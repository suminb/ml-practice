{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer with Multi-Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from common import load_data, score, write_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n",
    "row_count, feature_count = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to determine the number of neurons in a hidden layer? [Answers on StackOverflow](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw?newreg=91b1eff0e75a4cfdb984d99c21ccd384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=330, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(330), random_state=0)\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636190476190476"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(predictions, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning\n",
    "def tune_params(neuron_count):\n",
    "    # We'd like to try out a single hidden layer network first\n",
    "    model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(neuron_count,), random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions = model.predict(val_X)\n",
    "    return score(predictions, val_y)\n",
    "\n",
    "\n",
    "# test_X = pd.read_csv('test.csv')\n",
    "# for c in range(790, 1000, 10):\n",
    "#     mae = tune_params(c)\n",
    "#     print(c, mae)\n",
    "#     predictions = model.predict(test_X)\n",
    "#     write_output(predictions, f'output_nn_{c}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv('test.csv')\n",
    "predictions = model.predict(test_X)\n",
    "write_output(predictions, 'output_nn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n20 0.2\\n30 0.689238095238\\n40 0.861619047619\\n50 0.882666666667\\n60 0.885523809524\\n70 0.929238095238\\n80 0.942857142857\\n90 0.935523809524\\n100 0.94180952381\\n110 0.949904761905\\n120 0.944952380952\\n130 0.952\\n140 0.946\\n150 0.948952380952\\n160 0.952095238095\\n170 0.954952380952\\n180 0.957714285714\\n190 0.955142857143\\n200 0.96\\n210 0.958952380952\\n220 0.958571428571\\n230 0.958571428571\\n240 0.956952380952\\n241 0.959333333333\\n242 0.962666666667\\n243 0.959714285714\\n244 0.958095238095\\n245 0.958095238095\\n246 0.958095238095\\n247 0.959047619048\\n248 0.958095238095\\n249 0.958380952381\\n250 0.961619047619\\n251 0.959904761905\\n252 0.957142857143\\n253 0.960285714286\\n254 0.95619047619\\n255 0.958285714286\\n256 0.96219047619\\n257 0.95980952381\\n258 0.959619047619\\n259 0.960380952381\\n260 0.958761904762\\n270 0.959238095238\\n280 0.961333333333\\n290 0.95980952381\\n300 0.959047619048\\n310 0.960571428571\\n320 0.958952380952\\n321 0.958\\n322 0.958095238095\\n323 0.957238095238\\n324 0.958285714286\\n325 0.955428571429\\n326 0.957619047619\\n327 0.958095238095\\n328 0.957523809524\\n329 0.95819047619\\n330 0.963619047619\\n331 0.958\\n332 0.95980952381\\n333 0.958952380952\\n334 0.956380952381\\n335 0.959142857143\\n336 0.956952380952\\n337 0.960952380952\\n338 0.959142857143\\n339 0.958\\n340 0.956666666667\\n350 0.957904761905\\n360 0.960285714286\\n370 0.95980952381\\n380 0.958285714286\\n390 0.958095238095\\n400 0.959523809524\\n410 0.956666666667\\n420 0.95619047619\\n430 0.96\\n440 0.960285714286\\n450 0.957142857143\\n460 0.958095238095\\n480 0.96\\n490 0.958380952381\\n500 0.957428571429\\n510 0.95780952381\\n520 0.955333333333\\n530 0.957619047619\\n540 0.958761904762\\n550 0.958095238095\\n560 0.956857142857\\n570 0.959047619048\\n580 0.958571428571\\n590 0.958285714286\\n600 0.95819047619\\n610 0.955047619048\\n620 0.95619047619\\n630 0.957047619048\\n640 0.958571428571\\n650 0.956380952381\\n660 0.955619047619\\n670 0.959142857143\\n680 0.953428571429\\n690 0.955428571429\\n700 0.958952380952\\n710 0.955047619048\\n720 0.958761904762\\n730 0.955714285714\\n740 0.956380952381\\n750 0.956095238095\\n760 0.95580952381\\n770 0.959238095238\\n780 0.955619047619\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(56,), random_state=0)\n",
    "# mae = 1.1010476190476191\n",
    "\n",
    "# model = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(533,), random_state=0)\n",
    "# mae = 0.20038095238095238\n",
    "\n",
    "\n",
    "# model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(56,), random_state=0)\n",
    "# mae = 0.35295238095238096\n",
    "\n",
    "# model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(533,), random_state=0)\n",
    "# mae = 0.15266666666666667\n",
    "\n",
    "\n",
    "# model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(533, 75), random_state=0)\n",
    "# mae = 0.15161904761904763\n",
    "\n",
    "# model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(533, 100), random_state=0)\n",
    "# mae = 0.14066666666666666\n",
    "\n",
    "# model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(533, 150), random_state=0)\n",
    "# mae = 0.14361904761904762\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "20 0.2\n",
    "30 0.689238095238\n",
    "40 0.861619047619\n",
    "50 0.882666666667\n",
    "60 0.885523809524\n",
    "70 0.929238095238\n",
    "80 0.942857142857\n",
    "90 0.935523809524\n",
    "100 0.94180952381\n",
    "110 0.949904761905\n",
    "120 0.944952380952\n",
    "130 0.952\n",
    "140 0.946\n",
    "150 0.948952380952\n",
    "160 0.952095238095\n",
    "170 0.954952380952\n",
    "180 0.957714285714\n",
    "190 0.955142857143\n",
    "200 0.96\n",
    "210 0.958952380952\n",
    "220 0.958571428571\n",
    "230 0.958571428571\n",
    "240 0.956952380952\n",
    "241 0.959333333333\n",
    "242 0.962666666667\n",
    "243 0.959714285714\n",
    "244 0.958095238095\n",
    "245 0.958095238095\n",
    "246 0.958095238095\n",
    "247 0.959047619048\n",
    "248 0.958095238095\n",
    "249 0.958380952381\n",
    "250 0.961619047619\n",
    "251 0.959904761905\n",
    "252 0.957142857143\n",
    "253 0.960285714286\n",
    "254 0.95619047619\n",
    "255 0.958285714286\n",
    "256 0.96219047619\n",
    "257 0.95980952381\n",
    "258 0.959619047619\n",
    "259 0.960380952381\n",
    "260 0.958761904762\n",
    "270 0.959238095238\n",
    "280 0.961333333333\n",
    "290 0.95980952381\n",
    "300 0.959047619048\n",
    "310 0.960571428571\n",
    "320 0.958952380952\n",
    "321 0.958\n",
    "322 0.958095238095\n",
    "323 0.957238095238\n",
    "324 0.958285714286\n",
    "325 0.955428571429\n",
    "326 0.957619047619\n",
    "327 0.958095238095\n",
    "328 0.957523809524\n",
    "329 0.95819047619\n",
    "330 0.963619047619\n",
    "331 0.958\n",
    "332 0.95980952381\n",
    "333 0.958952380952\n",
    "334 0.956380952381\n",
    "335 0.959142857143\n",
    "336 0.956952380952\n",
    "337 0.960952380952\n",
    "338 0.959142857143\n",
    "339 0.958\n",
    "340 0.956666666667\n",
    "350 0.957904761905\n",
    "360 0.960285714286\n",
    "370 0.95980952381\n",
    "380 0.958285714286\n",
    "390 0.958095238095\n",
    "400 0.959523809524\n",
    "410 0.956666666667\n",
    "420 0.95619047619\n",
    "430 0.96\n",
    "440 0.960285714286\n",
    "450 0.957142857143\n",
    "460 0.958095238095\n",
    "480 0.96\n",
    "490 0.958380952381\n",
    "500 0.957428571429\n",
    "510 0.95780952381\n",
    "520 0.955333333333\n",
    "530 0.957619047619\n",
    "540 0.958761904762\n",
    "550 0.958095238095\n",
    "560 0.956857142857\n",
    "570 0.959047619048\n",
    "580 0.958571428571\n",
    "590 0.958285714286\n",
    "600 0.95819047619\n",
    "610 0.955047619048\n",
    "620 0.95619047619\n",
    "630 0.957047619048\n",
    "640 0.958571428571\n",
    "650 0.956380952381\n",
    "660 0.955619047619\n",
    "670 0.959142857143\n",
    "680 0.953428571429\n",
    "690 0.955428571429\n",
    "700 0.958952380952\n",
    "710 0.955047619048\n",
    "720 0.958761904762\n",
    "730 0.955714285714\n",
    "740 0.956380952381\n",
    "750 0.956095238095\n",
    "760 0.95580952381\n",
    "770 0.959238095238\n",
    "780 0.955619047619\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
